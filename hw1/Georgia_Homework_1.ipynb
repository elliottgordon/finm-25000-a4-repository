{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a97c4251",
      "metadata": {
        "id": "a97c4251"
      },
      "source": [
        "# Homework 1\n",
        "\n",
        "## FINM 25000 - 2025\n",
        "\n",
        "### UChicago Financial Mathematics\n",
        "\n",
        "* Mark Hendricks\n",
        "* hendricks@uchicago.edu\n",
        "\n",
        "## HBS Case\n",
        "\n",
        "### *The Harvard Management Company and Inflation-Indexed Bonds*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cef92ba4",
      "metadata": {
        "id": "cef92ba4"
      },
      "source": [
        "### Notation\n",
        "(Hidden LaTeX commands)\n",
        "\n",
        "$$\\newcommand{\\mux}{\\tilde{\\boldsymbol{\\mu}}}$$\n",
        "$$\\newcommand{\\wtan}{\\boldsymbol{\\text{w}}^{\\text{tan}}}$$\n",
        "$$\\newcommand{\\wtarg}{\\boldsymbol{\\text{w}}^{\\text{port}}}$$\n",
        "$$\\newcommand{\\mutarg}{\\tilde{\\boldsymbol{\\mu}}^{\\text{port}}}$$\n",
        "$$\\newcommand{\\wEW}{\\boldsymbol{\\text{w}}^{\\text{EW}}}$$\n",
        "$$\\newcommand{\\wRP}{\\boldsymbol{\\text{w}}^{\\text{RP}}}$$\n",
        "$$\\newcommand{\\wREG}{\\boldsymbol{\\text{w}}^{\\text{REG}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb624b4",
      "metadata": {
        "id": "cbb624b4"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31dfdadc",
      "metadata": {
        "id": "31dfdadc"
      },
      "source": [
        "# 1. HMC's Approach\n",
        "\n",
        "**Section 1 is not graded**, and you do not need to submit your answers. But you are encouraged to think about them, and we will discuss them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77eafffb",
      "metadata": {
        "id": "77eafffb"
      },
      "source": [
        "### 1.\n",
        "There are thousands of individual risky assets in which HMC can invest.  Explain why MV optimization across 1,000 securities is infeasible.\n",
        "\n",
        "### 2.\n",
        "Rather than optimize across all securities directly, HMC runs a two-stage optimization.\n",
        "1. They build asset class portfolios with each one optimized over the securities of the specific asset class.  \n",
        "2. HMC combines the asset-class portfolios into one total optimized portfolio.\n",
        "\n",
        "In order for the two-stage optimization to be a good approximation of the full MV-optimization on all assets, what must be true of the partition of securities into asset classes?\n",
        "\n",
        "### 3.\n",
        "Should TIPS form a new asset class or be grouped into one of the other 11 classes?\n",
        "\n",
        "### 4.\n",
        "Why does HMC focus on real returns when analyzing its portfolio allocation? Is this just a matter of scaling, or does using real returns versus nominal returns potentially change the MV solution?\n",
        "\n",
        "### 5.\n",
        "The case discusses the fact that Harvard places bounds on the portfolio allocation rather than implementing whatever numbers come out of the MV optimization problem.\n",
        "\n",
        "How might we adjust the stated optimization problem in the lecture notes to reflect the extra constraints Harvard is using in their bounded solutions given in Exhibits 5 and 6?\n",
        "\n",
        "### 6.\n",
        "Exhibits 5 shows zero allocation to domestic equities and domestic bonds across the entire computed range of targeted returns, (5.75% to 7.25%). Conceptually, why is the constraint binding in all these cases? What would the unconstrained portfolio want to do with those allocations and why?\n",
        "\n",
        "### 7.\n",
        "Exhibit 6 changes the constraints, (tightening them in most cases.) How much deterioration do we see in the mean-variance tradeoff that Harvard achieved?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d7adf1",
      "metadata": {
        "id": "f7d7adf1"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1614003",
      "metadata": {
        "id": "d1614003"
      },
      "source": [
        "# 2 Mean-Variance Optimization\n",
        "\n",
        "<i>This section is graded for a good-faith effort by your group. Submit your write-up- along with your supporting code. </i>\n",
        "\n",
        "### Data\n",
        "You will need the file in the github repo, `data/multi_asset_etf_data.xlsx`.\n",
        "- The time-series data gives monthly returns for the 11 asset classes and a short-term Treasury-bill fund return, (\"SHV\",) which we consider as the risk-free rate.\n",
        "- The data is provided in total returns, (in which case you should ignore the SHV column,) as well as excess returns, (where SHV has been subtracted from the other columns.)\n",
        "- These are nominal returns-they are not adjusted for inflation, and in our calculations we are not making any adjustment for inflation.\n",
        "- The exhibit data that comes via Harvard with the case is unnecessary for our analysis.\n",
        "\n",
        "### Model\n",
        "We are going to analyze the problem in terms of **excess** returns.\n",
        "- Thus, you will focus on the `Excess Returns` section of the lecture notes, especially the formulas on slide 50.\n",
        "- Be sure to use the`excess returns` tab of the data.\n",
        "\n",
        "### Format\n",
        "In the questions below, **annualize the statistics** you report.\n",
        "- Annualize the mean of monthly returns with a scaling of 12.\n",
        "- Annualize the volatility of monthly returns with a scaling of $\\sqrt{12}$\n",
        "- The Sharpe Ratio is the mean return divided by the volatility of returns. Accordingly, we can annualize the Sharpe Ratio with a scaling of $\\sqrt{12}$\n",
        "- Note that we are not scaling the raw timeseries data, just the statistics computed from it (mean, vol, Sharpe)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa47172",
      "metadata": {
        "id": "afa47172"
      },
      "source": [
        "### Footnotes\n",
        "\n",
        "#### Data File\n",
        "* The case does not give time-series data, so this data has been compiled outside of the case, and it intends to represent the main asset classes under consideration via various ETFs. For details on the specific securities/indexes, check the “Info” tab of the data.\n",
        "\n",
        "#### Risk-free rate\n",
        "* In the lecture-note we considered a constant risk-free rate. It is okay that our risk-free rate changes over time, but the assumption is that investors know it’s value one-period ahead of time. Thus, at any given point in time, it is a risk-free rate for the next period. (This is often discussed as the \"bank account\" or \"money market account\" in other settings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3962427",
      "metadata": {
        "id": "d3962427"
      },
      "source": [
        "### 1. Summary Statistics\n",
        "* Calculate and display the mean and volatility of each asset’s excess return. (Recall we use volatility to refer to standard deviation.)\n",
        "* Which assets have the best and worst Sharpe ratios? Recall that the Sharpe Ratio is simply the ratio of the mean-to-volatility of excess returns:\n",
        "$$\\text{sharpe ratio of investment }i = \\frac{\\mux_i}{\\sigma_i}$$\n",
        "\n",
        "Be sure to annualize all three statss (mean, vol, Sharpe).\n",
        "* mean is scaled by `12`\n",
        "* vol is scaled by `sqrt(12)`\n",
        "* Sharpe is scaled by `sqrt(12)`\n",
        "\n",
        "\n",
        "### 2. Descriptive Analysis\n",
        "* Calculate the correlation matrix of the returns. Which pair has the highest correlation? And the lowest?\n",
        "* How well have TIPS done in our sample? Have they outperformed domestic bonds? Foreign bonds?\n",
        "\n",
        "### 3. The MV frontier.\n",
        "* Compute and display the weights of the tangency portfolios: $\\wtan$.\n",
        "* Does the ranking of weights align with the ranking of Sharpe ratios?\n",
        "* Compute the mean, volatility, and Sharpe ratio for the tangency portfolio corresponding to\n",
        "$\\wtan$.\n",
        "\n",
        "### 4. TIPS\n",
        "Assess how much the tangency portfolio (and performance) change if...\n",
        "* TIPS are dropped completely from the investment set.\n",
        "* The expected excess return to TIPS is adjusted to be 0.0012 higher than what the historic sample shows.\n",
        "\n",
        "Based on the analysis, do TIPS seem to expand the investment opportunity set, implying that Harvard should consider them as a separate asset?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i8irDd3tbVA0",
      "metadata": {
        "id": "i8irDd3tbVA0"
      },
      "source": [
        "#Responses - Georgia Martin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-xizEY3wZxh0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xizEY3wZxh0",
        "outputId": "040a09e8-2831-408e-a402-5727c81ae4b3"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "hw1_data = pd.read_excel('/content/drive/My Drive/multi_asset_etf_data.xlsx', sheet_name='excess returns', index_col=0,parse_dates=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BvSFNbeVLveq",
      "metadata": {
        "id": "BvSFNbeVLveq"
      },
      "source": [
        "#1. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QSho1faSbU2B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "QSho1faSbU2B",
        "outputId": "5325ba8d-600c-4c53-f8d9-1eeff8dc380d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def performance_summary(df, adj_factor=12):\n",
        "  \"\"\"\n",
        "  Generates performance summary containing mean, volatility (vol), and Sharpe ratio (sharpe)\n",
        "  per each column in the given dataframe. Adj_factor is used to annualize the statistics.\n",
        "  \"\"\"\n",
        "  summary = {}\n",
        "  summary['mean'] = df.mean()*adj_factor #avg of each month * 12\n",
        "  summary['vol'] = df.std()*np.sqrt(adj_factor)\n",
        "  summary['sharpe'] = summary['mean']/summary['vol']\n",
        "  return pd.DataFrame(summary)\n",
        "metrics = performance_summary(hw1_data).sort_values('sharpe', ascending=False)\n",
        "metrics.head(11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4uN9oLAMqF6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4uN9oLAMqF6",
        "outputId": "643bcbe4-2bb4-4a7f-ea30-ee1c0a50430d"
      },
      "outputs": [],
      "source": [
        "print('Max sharpe ratio:', metrics['sharpe'].idxmax(), metrics['sharpe'].max())\n",
        "print('Min sharpe ratio:', metrics['sharpe'].idxmin(), metrics['sharpe'].min())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MUPNnnKNNsDJ",
      "metadata": {
        "id": "MUPNnnKNNsDJ"
      },
      "source": [
        "The maximum sharpe ratio belongs to SPY with a value of 0.9. The lowest belongs to BWX with a value of -0.09."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o_o9JM1-LzZM",
      "metadata": {
        "id": "o_o9JM1-LzZM"
      },
      "source": [
        "#2. Descriptive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lZn1rU3iMLz7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "lZn1rU3iMLz7",
        "outputId": "2f446397-a357-499e-d78b-336f3ce92f35"
      },
      "outputs": [],
      "source": [
        "corr_matrix = hw1_data.corr()\n",
        "corr_matrix.head(11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ERnvyLC8jCDM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERnvyLC8jCDM",
        "outputId": "05ac05a7-740e-4c86-a7a3-14b7efc50d5a"
      },
      "outputs": [],
      "source": [
        "corr_unstacked = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
        "                                           k=1).astype(bool)).unstack().dropna()\n",
        "highest_corr = corr_unstacked.idxmax(), corr_unstacked.max()\n",
        "lowest_corr = corr_unstacked.idxmin(), corr_unstacked.min()\n",
        "print(\"Highest correlation pair:\", highest_corr)\n",
        "print(\"Lowest correlation pair:\", lowest_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2DvNSqgpL2B8",
      "metadata": {
        "id": "2DvNSqgpL2B8"
      },
      "source": [
        "EFA and PSP have the highest correlation of 0.89, and IEF and DBC have the lowest correlation of -0.3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KCqwsi-6QZt7",
      "metadata": {
        "id": "KCqwsi-6QZt7"
      },
      "source": [
        "Comparing TIPS with IEF (Intermediate-Term U.S. Treasuries) and BWX (Foreign Government Bonds), we can see that TIPS delivered the strongest performance among the government bond ETFs analyzed. TIP had an annualized excess return of 2.05%, with a volatility of 5.11%, resulting in a Sharpe ratio of 0.40. In contrast, IEF had a lower return (1.64%) and a lower Sharpe ratio (0.26), indicating less attractive risk-adjusted performance. BWX, underperformed with a negative excess return of -0.77% and a negative Sharpe ratio (-0.09). This suggests that, in this sample, TIPS provided both higher returns and more efficient risk-reward tradeoffs than domestic and foreign government bonds."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8609d16",
      "metadata": {
        "id": "a8609d16"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mmDsJ9BwLlza",
      "metadata": {
        "id": "mmDsJ9BwLlza"
      },
      "source": [
        "# 3. The MV Fronteir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3hYZFkH5RAms",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "3hYZFkH5RAms",
        "outputId": "21a415eb-6948-4c05-d24a-689a57db65fb"
      },
      "outputs": [],
      "source": [
        "def tan_weights(df):\n",
        "  \"\"\"\n",
        "  Given a df with excess returns, computes the weights of the tangency\n",
        "  portfolios.\n",
        "  \"\"\"\n",
        "  mu = df.mean().values  # mean excess returns (monthly)\n",
        "  Sigma = df.cov().values  # covariance matrix (monthly)\n",
        "  ones = np.ones(len(mu))\n",
        "\n",
        "  inv_Sigma = np.linalg.inv(Sigma) #inverse\n",
        "\n",
        "  top = inv_Sigma @ mu\n",
        "  bottom = ones @ top\n",
        "  w_tan = top / bottom\n",
        "  tangency_weights = pd.Series(w_tan, index=df.columns)\n",
        "  tangency_df = tangency_weights.to_frame(name=\"Tangency Portfolio Weights\")\n",
        "  return tangency_df\n",
        "\n",
        "result = tan_weights(hw1_data)\n",
        "result.sort_values(by='Tangency Portfolio Weights', ascending=False).head(11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29cvYyytWhlZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "29cvYyytWhlZ",
        "outputId": "a3033d41-ade2-4dba-9461-800af80e0a07"
      },
      "outputs": [],
      "source": [
        "metrics.head(11)\n",
        "merged = pd.merge(metrics, result, left_index=True, right_index=True)\n",
        "merged.head(11)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HcML2R_UUYW-",
      "metadata": {
        "id": "HcML2R_UUYW-"
      },
      "source": [
        "While SPY remains the highest Sharpe ratio and tangency portfolio rate, the rest do not follow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XQOxc5lbXGKB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "XQOxc5lbXGKB",
        "outputId": "b122dd6f-a15d-4b4e-9765-ddb94c74a3bc"
      },
      "outputs": [],
      "source": [
        "def tan_metrics(df_ereturns,df):\n",
        "  \"\"\"\n",
        "  df_ereturns: original data\n",
        "  df: tangency portfolio weights meged with metrics\n",
        "\n",
        "  Given a dataframe of metrics including tangency weighted portfolios, computes\n",
        "  the mean, volatility, and Sharpe ratio wrt the tangency weight.\n",
        "  \"\"\"\n",
        "  w = df[\"Tangency Portfolio Weights\"].values\n",
        "  mu = df[\"mean\"].values\n",
        "  Sigma = df_ereturns.cov().values\n",
        "\n",
        "  assert w.shape[0] == Sigma.shape[0], \"Mismatch: weights and covariance matrix size\"\n",
        "\n",
        "  mean_portfolio = w @ mu\n",
        "  vol_portfolio = np.sqrt(w.T @ Sigma @ w)\n",
        "  sharpe_portfolio = mean_portfolio / vol_portfolio\n",
        "\n",
        "  return pd.DataFrame({\n",
        "      \"Annualized Mean\": [mean_portfolio],\n",
        "      \"Annualized Volatility\": [vol_portfolio],\n",
        "      \"Annualized Sharpe Ratio\": [sharpe_portfolio]\n",
        "    })\n",
        "tan_metrics(hw1_data, merged).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pd5q8DgzawG3",
      "metadata": {
        "id": "Pd5q8DgzawG3"
      },
      "source": [
        "For the tangency portfolio corresponding to\n",
        "$\\wtan$, the annualized mean is 1.16, the annualized volatility is 0.47, and the annualized sharpe ratio is 2.48."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i2ul8-NsbJej",
      "metadata": {
        "id": "i2ul8-NsbJej"
      },
      "source": [
        "#4. TIPS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hQwjMwyYb93N",
      "metadata": {
        "id": "hQwjMwyYb93N"
      },
      "source": [
        "Dropped TIPS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "opU7hR3zb03H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "opU7hR3zb03H",
        "outputId": "39078cc1-72d9-464f-af8c-68eea807075a"
      },
      "outputs": [],
      "source": [
        "dropped_tips = hw1_data.drop(columns=['TIP'])\n",
        "metrics_no_tips = performance_summary(dropped_tips).sort_values('sharpe', ascending=False)\n",
        "wtan_no_tip = tan_weights(dropped_tips)[\"Tangency Portfolio Weights\"]\n",
        "wtan_no_tip = pd.merge(metrics_no_tips, wtan_no_tip, left_index=True, right_index=True)\n",
        "tan_metrics(dropped_tips, wtan_no_tip).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2rGwqB9agsv1",
      "metadata": {
        "id": "2rGwqB9agsv1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "VTDt_AeEe0fo",
      "metadata": {
        "id": "VTDt_AeEe0fo"
      },
      "source": [
        "All aspects (mean, vol, and sharpe) are higher without TIPS."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aQFUQeNcqGh",
      "metadata": {
        "id": "0aQFUQeNcqGh"
      },
      "source": [
        "Adjusted TIPS:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jYmuaO18csXQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "jYmuaO18csXQ",
        "outputId": "0e03c6ef-b1da-4fb3-f3d5-2a15ba7fe73a"
      },
      "outputs": [],
      "source": [
        "adj_data = hw1_data.copy()\n",
        "adj_data['TIP'] = adj_data['TIP'] + 0.0012\n",
        "metrics_adj = performance_summary(adj_data).sort_values('sharpe', ascending=False)\n",
        "wtan_adj = tan_weights(adj_data)[\"Tangency Portfolio Weights\"]\n",
        "wtan_adjusted = pd.merge(metrics_adj, wtan_adj, left_index=True, right_index=True)\n",
        "tan_metrics(adj_data, wtan_adjusted).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dvPFX3TJgo5_",
      "metadata": {
        "id": "dvPFX3TJgo5_"
      },
      "source": [
        "All aspects (mean, vol, and sharpe) are lower with this adjusted TIPS."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59490d8",
      "metadata": {
        "id": "c59490d8"
      },
      "source": [
        "# 3. Allocations\n",
        "\n",
        "<i>This section is graded for a good-faith effort by your group. Submit your write-up- along with your supporting code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e5eb238",
      "metadata": {
        "id": "8e5eb238"
      },
      "source": [
        "* Continue with the same data file as the previous section.\n",
        "\n",
        "* Suppose the investor has a targeted mean excess return (per month) of $\\mutarg$ = 0.01."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d46b44c",
      "metadata": {
        "id": "1d46b44c"
      },
      "source": [
        "Build the following portfolios:\n",
        "\n",
        "#### Equally-weighted (EW)\n",
        "Rescale the entire weighting vector to have target mean $\\mutarg$. Thus, the $i$ element of the weight vector is,\n",
        "$$\\wEW_i = \\frac{1}{n}$$\n",
        "\n",
        "#### “Risk-parity” (RP)\n",
        "Risk-parity is a term used in a variety of ways, but here we have in mind setting the weight of the portfolio to be proportional to the inverse of its full-sample variance estimate. Thus, the $i$ element of the weight vector is,\n",
        "$$\\wRP_i = \\frac{1}{\\sigma_i^2}$$\n",
        "\n",
        "#### Mean-Variance (MV)\n",
        "As described in `Section 2`.\n",
        "\n",
        "\n",
        "### Comparing\n",
        "\n",
        "In order to compare all these allocation methods, rescale each weight vector, such that it has targeted mean return of $\\mutarg$.\n",
        "\n",
        "* Calculate the performance of each of these portfolios over the sample.\n",
        "* Report their mean, volatility, and Sharpe ratio.\n",
        "* How does performance compare across allocation methods?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pH2xczy8iIRw",
      "metadata": {
        "id": "pH2xczy8iIRw"
      },
      "source": [
        "#Responses - Georgia Martin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2x_Aw7KgiOlX",
      "metadata": {
        "id": "2x_Aw7KgiOlX"
      },
      "source": [
        "#Equally-weighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tva0p-YoiSsm",
      "metadata": {
        "id": "tva0p-YoiSsm"
      },
      "outputs": [],
      "source": [
        "target_mean = 0.01\n",
        "n = len(metrics)\n",
        "w_ew = pd.Series(1/n, index=metrics.index)\n",
        "\n",
        "# Portfolio mean\n",
        "mean_ew = (w_ew * metrics[\"mean\"]).sum()\n",
        "\n",
        "# Rescale weights so portfolio mean = target_mean\n",
        "w_ew_rescaled = w_ew * (target_mean / mean_ew)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ea828b",
      "metadata": {
        "id": "23ea828b"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JeI7S-Vmi2dy",
      "metadata": {
        "id": "JeI7S-Vmi2dy"
      },
      "source": [
        "#Risk-parity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kFfQZLLYi5pF",
      "metadata": {
        "id": "kFfQZLLYi5pF"
      },
      "outputs": [],
      "source": [
        "inv_var = 1 / (metrics[\"vol\"] ** 2)\n",
        "w_rp_raw = inv_var / inv_var.sum()\n",
        "\n",
        "# Portfolio mean\n",
        "mean_rp = (w_rp_raw * metrics[\"mean\"]).sum()\n",
        "\n",
        "# Rescale to target mean\n",
        "w_rp_rescaled = w_rp_raw * (target_mean / mean_rp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X2Tx1-VvjB48",
      "metadata": {
        "id": "X2Tx1-VvjB48"
      },
      "source": [
        "#Mean-Variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s2MRQCc2jFyB",
      "metadata": {
        "id": "s2MRQCc2jFyB"
      },
      "outputs": [],
      "source": [
        "w_mv = tan_weights(hw1_data)[\"Tangency Portfolio Weights\"]\n",
        "\n",
        "# Align weights index to stats_df to be safe\n",
        "w_mv = w_mv.reindex(metrics.index)\n",
        "\n",
        "# Compute portfolio mean\n",
        "mean_mv = (w_mv * metrics[\"mean\"]).sum()\n",
        "\n",
        "# Rescale to target mean\n",
        "w_mv_rescaled = w_mv * (target_mean / mean_mv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7Ywcx2dyjR7Q",
      "metadata": {
        "id": "7Ywcx2dyjR7Q"
      },
      "source": [
        "#Portfolio Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ehGs7TQLjUVS",
      "metadata": {
        "id": "ehGs7TQLjUVS"
      },
      "outputs": [],
      "source": [
        "def portfolio_performance(weights, metrics, returns):\n",
        "    weights = weights.reindex(metrics.index)\n",
        "    mu = metrics[\"mean\"].values\n",
        "    Sigma = returns.cov().values\n",
        "\n",
        "    w = weights.values\n",
        "    port_mean = w @ mu\n",
        "    port_vol = np.sqrt(w.T @ Sigma @ w)\n",
        "    port_sharpe = port_mean / port_vol\n",
        "\n",
        "    return pd.Series({\n",
        "        \"Mean\": port_mean,\n",
        "        \"Volatility\": port_vol,\n",
        "        \"Sharpe\": port_sharpe\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MMMWtsGHjm--",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMMWtsGHjm--",
        "outputId": "3597134c-fcba-4669-f469-5da3f972ddf9"
      },
      "outputs": [],
      "source": [
        "perf_ew = portfolio_performance(w_ew_rescaled, metrics, hw1_data)\n",
        "perf_rp = portfolio_performance(w_rp_rescaled, metrics, hw1_data)\n",
        "perf_mv = portfolio_performance(w_mv_rescaled, metrics, hw1_data)\n",
        "\n",
        "perf_df = pd.DataFrame({\n",
        "    \"Equally Weighted\": perf_ew,\n",
        "    \"Risk Parity\": perf_rp,\n",
        "    \"Mean Variance\": perf_mv\n",
        "}).T\n",
        "\n",
        "print(perf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1E3z6tZjxrV",
      "metadata": {
        "id": "e1E3z6tZjxrV"
      },
      "source": [
        "It makes sense that the MV portfolio has the highest Sharpe since it maximizes it by construction. The RP portfolio tends to control risk contributions, so it is a bit strange that it has the highest volatility. Equally weighted remains a simpler and more robust option.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a8443d",
      "metadata": {
        "id": "a2a8443d"
      },
      "source": [
        "# 4. EXTRA: Out-of-Sample Performance\n",
        "\n",
        "<i>This section is not graded, and you do not need to submit it. Still, we may discuss it in class, in which case, you would be expected to know it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e4b25b",
      "metadata": {
        "id": "24e4b25b"
      },
      "source": [
        "### 1. One-step Out-of-Sample (OOS) Performance\n",
        "Let’s divide the sample to both compute a portfolio and then check its performance out of sample.\n",
        "* Using only data through the end of `2022`, compute the weights built in Section 3.\n",
        "* Rescale the weights, (using just the in-sample data,) to set each allocation to have the same mean return of $\\mutarg$.\n",
        "* Using those weights, calculate the portfolio’s Sharpe ratio within that sample.\n",
        "* Again using those weights, (derived using data through `2022`,) calculate the portfolio’s OOS Sharpe ratio, which is based only on performance in `2023-2024`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2281c023",
      "metadata": {
        "id": "2281c023"
      },
      "source": [
        "### 2. Rolling OOS Performance\n",
        "\n",
        "Iterate the Out-of-Sample performance every year, not just the final year. Namely,\n",
        "* Start at the end of `2015`, and calculate the weights through that time. Rescale them using the mean returns through that time.\n",
        "* Apply the weights to the returns in the upcoming year, (`2016`.)\n",
        "* Step forward a year in time, and recompute.\n",
        "* Continue until again calculating the weights through `2023` and applying them to the returns in `2024`.\n",
        "\n",
        "Report the mean, volatility, and Sharpe from this dynamic approach for the following portfolios:\n",
        "* mean-variance (tangency)\n",
        "* equally-weighted\n",
        "* risk-parity\n",
        "* regularized"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99dc96f3",
      "metadata": {
        "id": "99dc96f3"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "665b0cf7",
      "metadata": {
        "id": "665b0cf7"
      },
      "source": [
        "# 5. EXTRA: Without a Riskless Asset\n",
        "\n",
        "<i>This section is not graded, and you do not need to submit it. Still, we may discuss it in class, in which case, you would be expected to know it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b1ff49",
      "metadata": {
        "id": "b8b1ff49"
      },
      "source": [
        "Re-do Section 2 above, but in the model without a risk-free rate.\n",
        "\n",
        "That is, build the MV allocation using the two-part formula in the `Mean-Variance` section of the notes.\n",
        "* This essentially substitutes the risk-free rate with the minimum-variance portfolio.\n",
        "* Now, the allocation depends nonlinearly on the target mean return, $\\mutarg$. (With a risk-free rate, we simply scale the weights up and down to achieve the mean return.)\n",
        "\n",
        "You will find that, conceptually, the answers are very similar."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d5734f",
      "metadata": {
        "id": "e7d5734f"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c864754",
      "metadata": {
        "id": "5c864754"
      },
      "source": [
        "# 6. EXTRA: Bayesian Allocation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f110c351",
      "metadata": {
        "id": "f110c351"
      },
      "source": [
        "Try the following allocation among the choices in `Section 3`..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59dae4fe",
      "metadata": {
        "id": "59dae4fe"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c70bb005",
      "metadata": {
        "id": "c70bb005"
      },
      "source": [
        "\n",
        "#### Regularized (REG)\n",
        "Much like the Mean-Variance portfolio, set the weights proportional to\n",
        "$$\\wREG \\sim \\widehat{\\Sigma}^{-1}\\mux$$\n",
        "but this time, use a regularized covariance matrix,\n",
        "$$\\widehat{\\Sigma} = \\frac{\\Sigma + \\Sigma_D}{2}$$\n",
        "where $\\Sigma_D$ denotes a *diagonal* matrix of the security variances, with zeros in the off-diagonals.\n",
        "\n",
        "Thus, $\\widehat{\\Sigma}$ is obtained from the usual covariance matrix, $\\Sigma$, but shrinking all the covariances to half their estimated values.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
